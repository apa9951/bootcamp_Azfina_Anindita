{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fdcc0a-e9d3-47eb-b7e9-7569c9363346",
   "metadata": {},
   "source": [
    "# Stage 04 - Data Acquisition and Ingestion\n",
    "## Lecture 04\n",
    "API Pull, Scraping, dll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fbb3695-111f-4c64-b248-7505e99a2008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded APLHAVANTAGE_API_KEY? True\n",
      "your_real_api_key_here\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, datetime as dt, csv, pathlib\n",
    "from typing import Dict, List\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#pathlib = library utk pilih direktori folder\n",
    "DATA_RAW = pathlib.Path(\"data/raw\")\n",
    "\n",
    "#mkdir untuk bikin folder\n",
    "DATA_RAW.mkdir(parents=True, exist_ok= True)\n",
    "\n",
    "load_dotenv()\n",
    "ALPHA_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "print(\"Loaded APLHAVANTAGE_API_KEY?\", bool(ALPHA_KEY))\n",
    "\n",
    "#print isi API nya\n",
    "print(ALPHA_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaf48987-9b35-4a75-aaa3-5ccb1aca5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: /Users/azfina/Documents/bootcamp_Azfina_Anindita/homework/Lecture04\n",
      "Files here: ['.DS_Store', '.env.txt', '.env', '.ipynb_checkpoints', 'Lecture04.ipynb', 'data']\n",
      "ALPHAVANTAGE_API_KEY: your_real_api_key_here\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1. Load .env explicitly (you can remove dotenv_path if .env is in same folder)\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# 2. See what files are in the current working directory\n",
    "print(\"Current working dir:\", os.getcwd())\n",
    "print(\"Files here:\", os.listdir())\n",
    "\n",
    "# 3. Try to read the key\n",
    "print(\"ALPHAVANTAGE_API_KEY:\", os.getenv(\"ALPHAVANTAGE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b6037-0d02-4083-a318-58b3d2d60b4d",
   "metadata": {},
   "source": [
    "# Helper Functions: Validation & Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4106db35-1798-43aa-98f9-0791519c4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_stamp():\n",
    "    return dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30bff27a-6c5a-4079-aa72-df2fcb647adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20250815-101147'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_stamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d9f3c3e-7683-427d-82b2-a2184fdc8dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25-08-15'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.now().strftime(\"%y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c8d88ae-e3fa-4dfd-a808-cf5ea13c0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_filename(prefix: str, meta:Dict [str, str]) -> str:\n",
    "    mid = \"_\".join([f\"{k}--{str(v).replace(' ', '_')[:20]}\" for k, v, in meta.items()])\n",
    "    return f\"{prefix}_{mid}_{safe_stamp()}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77828c62-36f4-42ff-993d-6e7cba0cd4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiki_param1--1_param2--2_20250815-101149.csv'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_filename (\"wiki\",{\"param1\":1,\"param2\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61c3097f-3bb3-4ccb-961e-7bed5ea9f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_df(df: pd.DataFrame, required_cols: List[str], dtypes_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    msgs = {}\n",
    "    \n",
    "    # Check for missing columns\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        msgs['missing_cols'] = f\"Missing columns: {missing}\"\n",
    "    \n",
    "    # Check column data types\n",
    "    for col, dtype in dtypes_map.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if dtype == 'datetime64[ns]':\n",
    "                    pd.to_datetime(df[col])\n",
    "                elif dtype == 'float':\n",
    "                    pd.to_numeric(df[col])\n",
    "            except Exception as e:\n",
    "                msgs[f'dtype_{col}'] = f\"Failed to coerce {col} to {dtype}: {e}\"\n",
    "    \n",
    "    # Count total NA values\n",
    "    na_counts = df.isna().sum().sum()\n",
    "    msgs['na_total'] = f\"Total NA values: {na_counts}\"\n",
    "    \n",
    "    return msgs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16009636-31ff-4bbd-a3c0-ed52d1d07dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['te', 'hu']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbered_list=[]\n",
    "for idx, i in enumerate([\"temperature\",\"humidity\"]):\n",
    "    numbered_list += [str(idx+1)+\". \"+str(i)]\n",
    "\n",
    "numbered_list\n",
    "\n",
    "#[str(idx+1)+\". \"+str(i) for idx, i in enumerate ([\"temperature\",\"humidity\"])\n",
    "\n",
    "[i[:2] for i in [\"temperature\",\"humidity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16b8ba8e-06a9-4c17-8495-4eebce04584f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 1}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test (x:str):\n",
    "    return x\n",
    "\n",
    "test({\"1\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0642892a-e0af-464a-a038-da8c968f991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 if 1 in [1,2] else 4\n",
    "#kalau 1 ada di 1,2 maka return 5. tapi kalau gak return 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fb696b3-6fe4-47c8-a960-5cd898725c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None if 1 not in [1,2] else 4\n",
    "#kalau 1 GAKada di 1,2 maka return None. tapi kalau gak return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3dc29a9b-5bc8-42c8-9a7d-ce2488804a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, False]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "\n",
    "#didalam range [1,2,3,4] kalau dia 1 maka TRUE tapi kalau bukan FALSE\n",
    "[True if c == 1 else False \n",
    " for c in [1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4498bb58-88f8-4b23-8e9a-61fcc87225f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, None, None]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[None if c not in [1,2] else c \n",
    "for c in [1,2,3,4]]\n",
    "\n",
    "#coba cek isi range [1,2,3,4] kalau bukan [1,2] maka return none; tapi kalau [1,2] maka return c (dirinya sendiri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6b42253f-b594-42a1-9162-46df6189b4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D3Q1QKM8OUW9K278'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALPHA_KEY=\"D3Q1QKM8OUW9K278\"\n",
    "ALPHA_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2f679e37-0f41-4ccd-a888-3e57645170b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Alpha Vantage: True\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Unexpected response keys: ['Information']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m js = r.json()\n\u001b[32m     20\u001b[39m key = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m js.keys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTime Series\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m key, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected response keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(js.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m series = js[key[\u001b[32m0\u001b[39m]]\n\u001b[32m     24\u001b[39m df_api = (pd.DataFrame(series).T\n\u001b[32m     25\u001b[39m           .rename_axis(\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m           .reset_index())\n",
      "\u001b[31mAssertionError\u001b[39m: Unexpected response keys: ['Information']"
     ]
    }
   ],
   "source": [
    "\n",
    "SYMBOL = \"AAPL\"\n",
    "\n",
    "use_alpha = bool(ALPHA_KEY)\n",
    "print(\"Using Alpha Vantage:\", use_alpha)\n",
    "\n",
    "if use_alpha:\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"outputsize\": \"compact\",\n",
    "        \"apikey\": ALPHA_KEY,\n",
    "        \"datatype\": \"json\"\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    \n",
    "    key = [k for k in js.keys() if \"Time Series\" in k]\n",
    "    assert key, f\"Unexpected response keys: {list(js.keys())}\"\n",
    "    \n",
    "    series = js[key[0]]\n",
    "    df_api = (pd.DataFrame(series).T\n",
    "              .rename_axis('date')\n",
    "              .reset_index())\n",
    "    \n",
    "    # keep a couple columns and coerce types\n",
    "    df_api = df_api[['date', '5. adjusted close']].rename(columns={'5. adjusted close': 'adj_close'})\n",
    "    df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "    df_api['adj_close'] = pd.to_numeric(df_api['adj_close'])\n",
    "else:\n",
    "    print(\"Alpha Vantage API key not found or empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5af98e2b-6bba-4bac-b975-1931608f0685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Information': 'Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints'}\n"
     ]
    }
   ],
   "source": [
    "print(js)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35d600-ac1d-4223-bc58-d4f99f558acb",
   "metadata": {},
   "source": [
    "## Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e050587a-d202-448a-96cd-d312a20ab867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape failed (demoing with inline HTML). 404 Client Error: Not Found for url: https://example.com/market-table\n"
     ]
    }
   ],
   "source": [
    "SCRAPE_URL = \"https://example.com/market-table\"  # replace with permitted page\n",
    "headers = {\"User-Agent\": \"AFE-Course-Notebook/1.0 (contact: instructor@example.edu)\"}\n",
    "\n",
    "try:\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr'):\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    # assume first row is header\n",
    "    header, *data = rows\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Scrape failed (demoing with inline HTML).\", e)\n",
    "    html = \"\"\"\n",
    "    <table>\n",
    "        <tr><th>Ticker</th><th>Price</th></tr>\n",
    "        <tr><td>AAA</td><td>101.2</td></tr>\n",
    "        <tr><td>BBB</td><td>98.7</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    rows = []\n",
    "    for tr in soup.find_all('tr'):\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    header, *data = rows\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e293f5a-861e-449e-a0a6-b99e1b5de502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # Country (or dependency) Population 2025 Yearly Change    Net Change  \\\n",
      "0  1                   India   1,463,865,525         0.89%    12,929,734   \n",
      "1  2                   China   1,416,096,094      â0.23%  â3,225,184   \n",
      "2  3           United States     347,275,807         0.54%     1,849,236   \n",
      "3  4               Indonesia     285,721,236         0.79%     2,233,305   \n",
      "4  5                Pakistan     255,219,554         1.57%     3,950,390   \n",
      "\n",
      "  Density (P/KmÂ²) Land Area (KmÂ²) Migrants (net) Fert. Rate Median Age  \\\n",
      "0              492        2,973,190     â495,753       1.94       28.8   \n",
      "1              151        9,388,211     â268,126       1.02       40.1   \n",
      "2               38        9,147,420      1,230,663       1.62       38.5   \n",
      "3              158        1,811,570      â39,509        2.1       30.4   \n",
      "4              331          770,880   â1,235,336        3.5       20.6   \n",
      "\n",
      "  Urban Pop % World Share  \n",
      "0       37.1%      17.78%  \n",
      "1       67.5%      17.20%  \n",
      "2       82.8%       4.22%  \n",
      "3       59.6%       3.47%  \n",
      "4       34.4%       3.10%  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Example URL (replace with a real page you are allowed to scrape)\n",
    "url = \"https://www.worldometers.info/world-population/population-by-country/\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # will raise an error if request fails\n",
    "\n",
    "# Parse HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the first table on the page\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Extract rows\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    cells = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Assume first row is header\n",
    "header, *data = rows\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "print(df.head())  # show first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51f1edb7-bfb1-4fb6-8f2c-5e3f1c74b3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol                        Name                     Price   Change  \\\n",
      "0   OPEN  Opendoor Technologies Inc.    3.4300+0.3900(+12.83%)  +0.3900   \n",
      "1   INTC           Intel Corporation        24.55+0.68(+2.87%)    +0.68   \n",
      "2     NU            Nu Holdings Ltd.       13.28+1.27(+10.62%)    +1.27   \n",
      "3   NVDA          NVIDIA Corporation       179.15-2.87(-1.57%)    -2.87   \n",
      "4   WULF               TeraWulf Inc.         8.84+0.13(+1.49%)    +0.13   \n",
      "\n",
      "  Change %    Volume Avg Vol (3M) Market Cap P/E Ratio(TTM) 52 WkChange %  \\\n",
      "0  +12.83%  236.331M     194.061M     2.524B             --       +69.83%   \n",
      "1   +2.87%  144.636M      87.982M   107.433B             --       +14.33%   \n",
      "2  +10.62%   71.496M      61.716M    64.092B          30.19       -14.76%   \n",
      "3   -1.57%   68.334M     183.388M     4.369T          57.98       +46.11%   \n",
      "4   +1.49%   52.207M      50.714M     3.465B             --      +116.13%   \n",
      "\n",
      "  52 Wk Range  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4              \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Yahoo Finance Most Active Stocks page\n",
    "url = \"https://finance.yahoo.com/most-active\"\n",
    "\n",
    "# Request the page\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Pretend to be a browser\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the table\n",
    "table = soup.find(\"table\")  # Yahoo's first table on the page\n",
    "\n",
    "# Extract data\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    cells = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Assume first row is header\n",
    "header, *data = rows\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c1e50a6a-cac2-40fc-a2cc-0e0eeb2c3e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data from Alpha Vantage API\n",
      "         date      Open      High       Low     Close     Volume\n",
      "0  2025-08-14  234.0550  235.1200  230.8500  232.7800   51916275\n",
      "1  2025-08-13  231.0700  235.0000  230.4300  233.3300   69878546\n",
      "2  2025-08-12  228.0050  230.8000  227.0700  229.6500   55672301\n",
      "3  2025-08-11  227.9200  229.5600  224.7600  227.1800   61806132\n",
      "4  2025-08-08  220.8300  231.0000  219.2500  229.3500  113853967\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# --- SETTINGS ---\n",
    "symbol = \"AAPL\"  # Example: Apple\n",
    "ALPHA_KEY = \"YOUR_ALPHA_VANTAGE_API_KEY\"\n",
    "\n",
    "# --- TRY ALPHA VANTAGE FIRST ---\n",
    "try:\n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={ALPHA_KEY}\"\n",
    "    r = requests.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    key = [k for k in js.keys() if \"Time Series\" in k]\n",
    "    assert key, f\"Unexpected response keys: {list(js.keys())}\"\n",
    "\n",
    "    series = js[key[0]]\n",
    "    df_api = (\n",
    "        pd.DataFrame(series).T\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\n",
    "            \"1. open\": \"Open\",\n",
    "            \"2. high\": \"High\",\n",
    "            \"3. low\": \"Low\",\n",
    "            \"4. close\": \"Close\",\n",
    "            \"5. volume\": \"Volume\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    print(\"✅ Data from Alpha Vantage API\")\n",
    "    print(df_api.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"⚠ API failed, falling back to Yahoo Finance scrape:\", e)\n",
    "\n",
    "    # --- FALLBACK: SCRAPE YAHOO FINANCE ---\n",
    "    scrape_url = f\"https://finance.yahoo.com/quote/{symbol}/history\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    resp = requests.get(scrape_url, headers=headers, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    \n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    \n",
    "    header, *data = rows\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "    print(\"✅ Data from Yahoo Finance scraping\")\n",
    "    print(df_scrape.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "60c9363c-ec45-4d81-9e07-5e7fe2ab7c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "def my_func():\n",
    "    try:\n",
    "        print(\"hello\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "x = []\n",
    "\n",
    "try:\n",
    "    print(\"bye\")\n",
    "    my_func()\n",
    "except Exception as e:\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072e453-cef0-4b3e-83df-aee52fd6099c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
